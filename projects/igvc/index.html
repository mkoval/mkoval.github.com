---
layout: default
title: IGVC - Projects - Michael Koval
category: projects
---

<h2>Rutgers Navigator, 2011 IGVC</h2>
<p>The <em>Navigator</em> is a three-wheeled, differential-drive robotics
research platform that was designed, built, and programmed entirely by the
Rutgers University <a href="http://ieee.rutgers.edu">IEEE Student Branch</a>
for competing in the 2011 <a href="http://www.igvc.org">Intelligent Ground
Vehicle Competition</a>. I was responsible for designing the Navigator's camera
system and writing the computer vision software necessary for processing its
output.</p>

<div class="photos">
	<div class="figure" style="width:320px;">
		<img src="images/navi_cad.png" width="318" height="224"
		     alt="CAD render of the Navigator"/>
		<span>Textured CAD Render of the Navigator</span>
	</div>
	<div class="figure" style="width:320px;">
		<img src="images/navi_pic.jpg" width="318" height="223"
		     alt="photo of the Navigator"/>
		<span>Photo of the Navigator</span>
	</div>
</div>

<p>All of the content on this page is discussed in more depth in our official
2011 IGVC Design Report or my capstone design final report:</p>
<ul>
	<li><a href="files/igvc_presentation.pdf" class="pdf"><em>Navigator</em> Design Presentation: 2011 IGVC</a> by Adam Stambler, Michael Koval, and Peter Vasilnak</li>
	<li><a href="files/igvc_design.pdf" class="pdf"><em>Navigator</em> Design Report: 2011 IGVC</a> by Adam Stambler, Michael Koval, and Peter Vasilnak</li>
	<li><a href="files/igvc_capstone.pdf" class="pdf">Vision-Based Autonomous Ground Vehicle Navigation</a> by Michael Koval</li>
</ul>

<h2>Camera System</h2>

<img src="images/cameras.jpg" width="240" height="209" style="float:right;"
     alt="stereo and wheel cameras superimposed over a ghost CAD render of the Navigator"/>

<p>There are a total of five <a
href="http://en.wikipedia.org/wiki/PlayStation_Eye">PlayStation Eye</a> cameras
mounted in custom polycarbonate cases and attached to the Navigator's aluminum
frame.  Originally intended as a peripheral for the PlayStation 3 game console,
these cameras can be used as inexpensive USB web-cameras with a
disproportionate number of high-end features:</p>

<ul>
	<li>Video4Linux Kernel Support</li>
	<li>High Framerate (640 &times; 320 @ 60 Hz, 320 &times; 280 @ 120 Hz)</li>
	<li>Wide-Angle Lens (75&deg; HFOV)</li>
	<li>Hardware Synchronization (with modifications)</li>
	<li>Uncompressed Video.</li>
</ul>

<p>Three of these cameras form a custom trinocular vision system that is
mounted to the front of the Navigator. All three of these cameras are
hardware-synchronized to share the same clock, which enables accurate stereo
reconstruction while moving at high speeds. Synchronized frames from these
cameras are captured at 15 Hz with a resolution of 640 &times; 480 and all
compression disabled. The remaining two cameras are mounted above the robot's
front two wheels, asynchronously capture images at 15 Hz, and use a lower
resolution of 320 &times; 240. Despite their proximity to the ground, these
five cameras have a composite field of view of nearly 170&deg;.</p>

<h2>Stereo Obstacle Detection</h2>
<p>Selecting the optimal baseline for a stereo system is a balance of two
opposing, but equally important, factors: field of view and maximum range.
Decreasing the baseline increases the shared field of view of the two cameras
at the cost of a shorter maximum range. Conversely, increasing the baseline
decreases the aggregate field of view, but yields an increased maximum range
and better precision at each visible distance. Using a trinocular stereo system
instead of a standard binocular system allows the software to get the
advantages of two baselines with the addition of only one camera.</p>

<div class="photos" style="margin:1.5em 0;">
	<div class="figure">
		<img src="images/stereo_rgb.jpg"  width="200" height="150"
		     alt="original color image"/>
		<span>Source Image</span>
	</div>
	<div class="figure">
		<img src="images/stereo_narrow.jpg" width="200" height="150"
		     alt="narrow baseline disparity map"/>
		<span>Narrow Disparity Map</span>
	</div>
	<div class="figure">
		<img src="images/stereo_wide.jpg" width="200" height="150"
		     alt="narrow baseline disparity map"/>
		<span>Wide Disparity Map</span>
	</div>
</div>

<p>The narrow pair has a baseline of approximately 10 cm and uses the left and
middle cameras. Conversely, the wide pair has a baseline of approximately 20 cm
and uses the left and right cameras. By using the narrow baselines for nearby
points and the wide baseline for more distance points, this trinocular stereo
system combines the small minimum range of the narrow baseline with the better
accuracy and maximum range of the wide baseline. Practically, this is
implemented by calibrating the two baselines independently and processing the
two stereo pairs in parallel.</p>

<p>Once images from both baselines are processed for point correspondences, the
resulting pointclouds are merged. By assuming that ground is the dominant plane
in the composite point cloud, the navigator fits a planar model to the data
using RANSAC. This model is smoothed by a low-pass filter and is subject to
several heuristics that verify that the plane was incorrectly fit to a planar
obstacle. Points that are sufficiently far from the fitted ground plane are
passed through a statistical outlier detection filter and are assumed to be
obstacles.</p>

<h2>Lane Tracking</h2>
<p>Not only must the Navigator see road obstacles, it must also be capable of
detecting the white painted lines that deﬁne the edges of the Autonomous
Challenge obstacle course. Analysis of the 2010 IGVC results sho that accurate
line detection is crucial to performing well: a mere 2 of 28 teams lasted the
full duration of the Autonomous Challenge without being disqualified. To avoid
suffering the same fate, the Navigator simultaneously searches for lines in
images from three cameras: the front-mounted stereo camera, the left wheel
camera, and the right wheel camera. By running the entire processing pipeline
at 10-15 Hz, the Navigator is able to detect lines within in a 180&deg; ﬁeld of
view with less than 100 ms of latency</p>

<div class="photos">
	<div class="figure">
		<img src="images/line_rgb.jpg" width="200" height="150"
		     alt="original color image"/>
		<span>Source Image</span>
	</div>
	<div class="figure">
		<img src="images/line_pulse.jpg" width="200" height="150"
		     alt="matched pulse-width filter response"/>
		<span>Pulse-Width Filter Response</span>
	</div>
	<div class="figure">
		<img src="images/line_nonmax.jpg" width="200" height="150"
		     alt="output of non-maximal supression"/>
		<span>Non-Maximal Suppression</span>
	</div>
</div>

<p>Lines in each of the three images are independently identified using a
three-stage algorithm. Immediately upon receiving a new frame from the camera,
the original color image undergoes a color space transformation that emphasizes
white regions of the image. The resulting monochromatic image is then searched
for regions of high intensity that match the expected width of the line using a
matched pulse-width filter. The output of the matched pulse-width filter is
reduced to a set of candidate line points through non-maximal suppression and
the local maxima from all three images are projected onto the ground plane in a
common coordinate frame.</p>

<h2>Acknowledgements</h2>
<p>The Rutgers IGVC team would like to extend our gratitude to everyone that
has helped us along the way this year. We could not have finished the robot
without the generous support of our sponsors: Dr. Stuart Shalat of the <a
href="http://eohsi.rutgers.edu/">Advanced Robotics Environmental and Assessment
Lab</a> (EOHSI), <a href="http://www.optimabatteries.com/">Optima
Batteries</a>, <a href="http://www.novatel.com/">Novatel</a>, <a
href="http://www.omnistar.com/">Omnistar</a>, <a
href="https://github.com/">Github</a>, <a href="http://www.ieee.org/">IEEE</a>,
<a href="http://www.8020.net/">80/20, Inc.</a>, the <a
href="http://knottsco.com/">Knotts Company</a>, the <a
href="http://egc.rutgers.edu/">Rutgers Engineering Governing Council</a>, <a
href="http://www.cs.rutgers.edu/~mlittman/">Dr. Michael Littman</a> of the <a
href="http://cs.rutgers.edu/rl3">Rutgers Laboratory for Real-life Reinforcement
learning</a> (RL<sup>3</sup>), and <a
href="http://www.cs.rutgers.edu/~dnm/">Dr. Dimitris Metaxas</a> of the <a
href="http://cbim.rutgers.edu/">Rutgers Computational Biomedicine Imaging and
Modeling Center</a> (CBIM). In addition to our sponsors, we would like to thank
<a href="http://www.ece.rutgers.edu/~kdana/">Dr. Kristin Dana</a> for answering
all of our questions and <a href="http://www.winlab.rutgers.edu/~spasojev/">Dr.
Predrag Spasojevic</a> for serving as the <a
href="http://ieee.rutgers.edu">Rutgers IEEE Student Branch</a>’s faculty
advisor. Finally, we would like to thank Joe Lippencott, Steve Orbine, John
Scafidi, and the departments of <a
href="http://www.ece.rutgers.edu">Electrical</a>, <a
href="http://ie.rutgers.edu/">Industrial</a>, and <a
href="http://mech.rutgers.edu/">Mechanical</a> engineering for their continued
support.</p>

<p>Also, thanks to all of the <a href="http://ieee.rutgers.edu">IEEE Student
Branch</a> members that contributed to the Navigator: Adam Stambler, Peter
Vasilnak, Cody Schaffer, Elie Rosen, Nitish Thatte, Rohith Dronadula, and Siva
Yedithi.</p>
